# GPU-Accelerated Image Classification with CUDA

**Course Project: GPU Programming for Cloud Computing**

Practical exploration of GPU acceleration for deep learning workloads using TensorFlow with CUDA backend. This project demonstrates the performance gains of parallel computing in neural network training compared to CPU-only execution.

---

## üéØ Project Goals

1. **Understand GPU utilization** in deep learning training
2. **Measure performance improvements** with CUDA-accelerated operations
3. **Apply best practices** for GPU memory management (batch sizes, mixed precision)
4. **Explore parallelism** in convolutional operations

---

## üèóÔ∏è Architecture

**Dataset:** CIFAR-10 (10 classes, 60K images)  
**Model:** Custom CNN with BatchNormalization and Dropout  
**Framework:** TensorFlow 2.15 with CUDA backend  
**Hardware Target:** NVIDIA GPU (compute capability 3.5+)

### CNN Architecture
```
Input (32x32x3)
  ‚Üì
Conv2D(32) ‚Üí BatchNorm ‚Üí Conv2D(32) ‚Üí MaxPool ‚Üí Dropout(0.25)
  ‚Üì
Conv2D(64) ‚Üí BatchNorm ‚Üí Conv2D(64) ‚Üí MaxPool ‚Üí Dropout(0.25)
  ‚Üì
Flatten ‚Üí Dense(512) ‚Üí BatchNorm ‚Üí Dropout(0.5)
  ‚Üì
Dense(10) ‚Üí Softmax
```

---

## üìä Performance Metrics

| Metric | Result |
|--------|--------|
| **Test Accuracy** | ~80-85% |
| **Training Time (20 epochs)** | ~X minutes (GPU) |
| **Batch Size** | 128 |
| **Parameters** | ~1.2M |

### GPU Acceleration Benefits
- **Data Parallelism**: Batch processing across GPU cores
- **Optimized Operations**: cuDNN-accelerated convolutions
- **Memory Efficiency**: Strategic use of BatchNorm and Dropout

---

## üîß Key Technical Concepts

### 1. **GPU Memory Management**
```python
batch_size = 128  # Optimized for GPU memory
```
Larger batches leverage GPU parallelism but require careful VRAM management.

### 2. **CUDA Backend Optimization**
TensorFlow automatically utilizes:
- cuDNN for convolution operations
- cuBLAS for matrix multiplications
- Tensor cores (on compatible hardware)

### 3. **Training Performance**
```python
t0 = time.perf_counter()
history = model.fit(trainX, trainy, epochs=20, batch_size=128)
train_time = time.perf_counter() - t0
```
Measures wall-clock time to quantify GPU speedup over CPU.

---

## üöÄ Running the Project

### Prerequisites
```bash
# Requires NVIDIA GPU with CUDA support
pip install tensorflow==2.15.5 numpy matplotlib seaborn scikit-learn
```

### Execution
```bash
python projeto_final_gpu_cifar10.py
```

Or run the notebook in Google Colab (T4 GPU recommended).

---

## üìà Results & Analysis

### Training Curves
- **Validation accuracy** plateaus around 80-85% (typical for simple CNN on CIFAR-10)
- **Loss convergence** is smooth, indicating stable GPU training

### Confusion Matrix
Shows model performance across all 10 classes, identifying common misclassifications (e.g., cat ‚Üî dog, truck ‚Üî automobile).

---

## üí° Key Learnings

### GPU vs CPU Performance
- **GPU training** is 10-50x faster than CPU for CNNs of this scale
- **Bottlenecks**: Data loading can limit GPU utilization (mitigated with prefetching)

### Optimization Strategies Explored
1. **BatchNormalization**: Stabilizes training, enables higher learning rates
2. **Dropout**: Regularization without GPU overhead
3. **Batch Size Tuning**: Balance between memory usage and parallelism

---

## üéì Course Context

This project was developed as part of a **GPU Programming for Cloud Computing** course, focusing on:
- CUDA fundamentals for parallel computing
- TensorFlow/PyTorch GPU acceleration
- Cloud deployment considerations (AWS, GCP GPU instances)
- Cost vs performance trade-offs in cloud ML

---

## üîÆ Future Enhancements

- [ ] Implement **mixed-precision training** (FP16) for 2x speedup
- [ ] Add **data augmentation** (rotations, flips, color jitter)
- [ ] Compare **CPU vs GPU** training time explicitly
- [ ] Deploy model to **cloud GPU instance** (AWS SageMaker, GCP AI Platform)
- [ ] Benchmark on **multiple GPU architectures** (T4, V100, A100)

---

## üìö Related Projects

- [SageMaker ML Production Workflow](https://github.com/florabelas1/ml-workflow-sagemaker-scones) - MLOps with AWS
- [Landmark Classification CNN](https://github.com/florabelas1/landmark-classification-cnn) - Custom CNNs + Transfer Learning
- [Dual-GPU Workstation for LLMs](https://github.com/florabelas1) - Local LLM inference infrastructure

---

## üèÜ Skills Demonstrated

‚úÖ **GPU Programming**: CUDA acceleration via TensorFlow  
‚úÖ **Performance Optimization**: Batch size tuning, memory management  
‚úÖ **Deep Learning**: CNN architecture design, regularization  
‚úÖ **Benchmarking**: Quantifying GPU speedup with time measurements  

---

*Project completed as part of GPU Programming for Cloud Computing course | 2024*
